{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import csv\n",
    "import re\n",
    "import requests\n",
    "import glob\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for the raw data, the key is the wbesite containing the raw data and the key is the ID relating to the table required on the web page\n",
    "\n",
    "def league_stats(seasons):\n",
    "\n",
    "    keywords_1 = ['stats', 'keepers', 'shooting', 'misc', 'passing', 'defense']\n",
    "    lst_of_dicts = []\n",
    "\n",
    "    for season in seasons:\n",
    "        for keyword in keywords_1:\n",
    "\n",
    "            if keyword=='stats':\n",
    "                team_id = {'id': {'stats_squads_standard_for'}}\n",
    "                player_id = {'id':{'stats_standard'}}\n",
    "\n",
    "            elif keyword=='keepers':\n",
    "                team_id = {'id': {'stats_squads_keeper_for'}}\n",
    "                player_id = {'id':{'stats_keeper'}}\n",
    "\n",
    "            elif keyword=='shooting':\n",
    "                team_id = {'id': {'stats_squads_shooting_for'}}\n",
    "                player_id = {'id':{'stats_shooting'}}\n",
    "\n",
    "            elif keyword=='misc':\n",
    "                team_id = {'id': {'stats_squads_misc_for'}}\n",
    "                player_id = {'id':{'stats_misc'}}\n",
    "\n",
    "            elif keyword=='passing':\n",
    "                team_id = {'id': {'stats_squads_passing_for'}}\n",
    "                player_id = {'id':{'stats_passing'}}\n",
    "\n",
    "            elif keyword=='defense':\n",
    "                team_id = {'id': {'stats_squads_defense_for'}}\n",
    "                player_id = {'id':{'stats_defense'}}\n",
    "\n",
    "            else:\n",
    "                None\n",
    "\n",
    "            if keyword in ('passing','defense'):\n",
    "                spa_urls = f'https://fbref.com/en/comps/12/{season}/{keyword}/{season}-La-Liga-Stats'\n",
    "                fra_urls = f'https://fbref.com/en/comps/13/{season}/{keyword}/{season}-Ligue-1-Stats'\n",
    "                eng_urls = f'https://fbref.com/en/comps/9/{season}/{keyword}/{season}-Premier-League-Stats'\n",
    "                ger_urls = f'https://fbref.com/en/comps/20/{season}/{keyword}/{season}-Bundesliga-Stats'\n",
    "                por_urls = f'https://fbref.com/en/comps/32/{season}/{keyword}/{season}-Primeira-Liga-Stats'\n",
    "                ita_urls = f'https://fbref.com/en/comps/11/{season}/{keyword}/{season}-Serie-A-Stats'\n",
    "\n",
    "\n",
    "                dict = {spa_urls: [team_id,player_id], fra_urls: [team_id,player_id], eng_urls: [team_id,player_id], \n",
    "                          ger_urls: [team_id,player_id], por_urls: [team_id,player_id], ita_urls: [team_id,player_id]\n",
    "                        }\n",
    "  \n",
    "                \n",
    "                lst_of_dicts.append(dict)\n",
    "\n",
    "            else:\n",
    "                spa_urls = f'https://fbref.com/en/comps/12/{season}/{keyword}/{season}-La-Liga-Stats'\n",
    "                fra_urls = f'https://fbref.com/en/comps/13/{season}/{keyword}/{season}-Ligue-1-Stats'\n",
    "                eng_urls = f'https://fbref.com/en/comps/9/{season}/{keyword}/{season}-Premier-League-Stats'\n",
    "                ger_urls = f'https://fbref.com/en/comps/20/{season}/{keyword}/{season}-Bundesliga-Stats'\n",
    "                por_urls = f'https://fbref.com/en/comps/32/{season}/{keyword}/{season}-Primeira-Liga-Stats'\n",
    "                ita_urls = f'https://fbref.com/en/comps/11/{season}/{keyword}/{season}-Serie-A-Stats'\n",
    "                bul_urls = f'https://fbref.com/en/comps/67/{season}/{keyword}/{season}-Bulgarian-First-League-Stats'\n",
    "                aus_urls = f'https://fbref.com/en/comps/56/{season}/{keyword}/{season}-Austrian-Bundesliga-Stats'\n",
    "                dan_urls = f'https://fbref.com/en/comps/50/{season}/{keyword}/{season}-Danish-Superliga-Stats'\n",
    "                bel_urls = f'https://fbref.com/en/comps/37/{season}/{keyword}/{season}-Belgian-Pro-League-Stats'           \n",
    "                gre_urls = f'https://fbref.com/en/comps/27/{season}/{keyword}/{season}-Super-League-Greece-Stats'           \n",
    "                net_urls = f'https://fbref.com/en/comps/23/{season}/{keyword}/{season}-Eredivisie-Stats'\n",
    "                pol_urls = f'https://fbref.com/en/comps/36/{season}/{keyword}/{season}-Ekstraklasa-Stats'           \n",
    "                ser_urls = f'https://fbref.com/en/comps/54/{season}/{keyword}/{season}-Serbian-SuperLiga-Stats'\n",
    "                tur_urls = f'https://fbref.com/en/comps/26/{season}/{keyword}/{season}-Super-Lig-Stats'\n",
    "\n",
    "\n",
    "                dict = {spa_urls: [team_id,player_id], fra_urls: [team_id,player_id], eng_urls: [team_id,player_id], \n",
    "                        ger_urls: [team_id,player_id], por_urls: [team_id,player_id], ita_urls: [team_id,player_id], \n",
    "                        bul_urls: [team_id,player_id], aus_urls: [team_id,player_id], dan_urls: [team_id,player_id], \n",
    "                        bel_urls: [team_id,player_id], gre_urls: [team_id,player_id], net_urls: [team_id,player_id], \n",
    "                        pol_urls: [team_id,player_id], ser_urls: [team_id,player_id], tur_urls: [team_id,player_id]\n",
    "                        }\n",
    "                \n",
    "                lst_of_dicts.append(dict)\n",
    "                \n",
    "\n",
    "    return lst_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping player data\n",
    "\n",
    "# Set up Selenium options\n",
    "options = Options()\n",
    "options.headless = True  # Run in headless mode (no GUI)\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "csv_files = glob.glob(f'**.csv')\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "season_lst = ['2017-2018', '2018-2019', '2019-2020']\n",
    "\n",
    "# Fetch the webpages for the desired seasons\n",
    "for data_dict in league_stats(season_lst):\n",
    "    \n",
    "    for page, class_id in data_dict.items():\n",
    "        for id in class_id:\n",
    "            print(page, id)\n",
    "\n",
    "        # Creating output csv name\n",
    "            def create_filename(url, id):\n",
    "                # Extract the relevant parts of the URL\n",
    "                match = re.search(r'/(\\d{4}-\\d{4})/([^/]+)/(\\d{4}-\\d{4}-[^/]+)', url)\n",
    "                table_id = re.search(r\"'id': {'([^']*)'}\", id)\n",
    "\n",
    "                if match and id:\n",
    "                    category = match.group(2).title()\n",
    "                    stats = match.group(3).replace('-', '_')[:-6]\n",
    "                    table = table_id.group(1)\n",
    "\n",
    "                    if '_for' not in table:\n",
    "                        table_type = 'Player'\n",
    "                    else:\n",
    "                        table_type = 'Team'\n",
    "\n",
    "                    # Format the filename\n",
    "                    filename = f\"{stats}_{category}_{table_type}.csv\"\n",
    "\n",
    "                    return filename\n",
    "                else:\n",
    "                    return None\n",
    "\n",
    "            csv_name = create_filename(page, str(id))\n",
    "\n",
    "            if csv_name in csv_files:\n",
    "                print('Dataset already exsits')\n",
    "\n",
    "            else:\n",
    "                driver.get(page)\n",
    "\n",
    "                # Get the page source after JavaScript has rendered\n",
    "                html = driver.page_source\n",
    "\n",
    "                # Parse the HTML content with BeautifulSoup\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "                # Find the table with the specified class\n",
    "                table = soup.find('table', id)\n",
    "\n",
    "                # Extract the data from the table\n",
    "                if table:\n",
    "\n",
    "                    # Find the thead element\n",
    "                    thead = table.find('thead')\n",
    "                    # Extract column headers with aria-label\n",
    "                    headers = thead.find_all('th')\n",
    "                    header_labels = [header.get('aria-label', header.text).strip() for header in headers if header.get('aria-label', header.text).strip()]\n",
    "                            \n",
    "                    # Use a set to remove duplicates, required due to multiple headers in the table across the web page\n",
    "                    unique_header_labels = list(dict.fromkeys(header_labels))\n",
    "\n",
    "                    # Removing unwanted blank fields scraped incorrectly\n",
    "                    if 'Rk' in unique_header_labels:\n",
    "                        filtered_lst = [header for header in header_labels if not (header.isdigit())][1:]\n",
    "                    else:\n",
    "                        filtered_lst = [header for header in header_labels if not (header.isdigit())]\n",
    "\n",
    "                    if '/stats/' in page and 'npxG + xAG/90' not in filtered_lst:\n",
    "                        if 'Matches' in filtered_lst:\n",
    "                            filtered_lst.remove('Matches')\n",
    "                            filtered_lst[-1]='npxG + xAG/90'\n",
    "                            filtered_lst.append('Matches')\n",
    "\n",
    "                        else:\n",
    "                            filtered_lst[-1]='npxG + xAG/90'\n",
    "\n",
    "                    rows = table.find_all('tr')\n",
    " \n",
    "                    data = []\n",
    "\n",
    "                    for row in rows:\n",
    "\n",
    "                        columns = row.find_all('td')\n",
    "                        \n",
    "                        # Check if <th> and <a> tags exist\n",
    "                        team_name_tag = row.find('th')\n",
    "                        if team_name_tag and 'Team' in csv_name:\n",
    "\n",
    "                            team_name_link = team_name_tag.find('a')\n",
    "                            team_name = team_name_link.text if team_name_link else 'N/A'\n",
    "                            row_data = [team_name] + [column.text for column in columns]\n",
    "                            data.append(row_data)\n",
    "\n",
    "                        else:\n",
    "                            \n",
    "                            row_data = [column.text for column in columns]\n",
    "                            data.append(row_data)\n",
    "\n",
    "                    # Write data to CSV\n",
    "                    with open(csv_name, 'w', newline='', encoding='utf-8') as file:\n",
    "                        \n",
    "                        writer = csv.writer(file)\n",
    "                        # Write header labels first\n",
    "                        writer.writerow(filtered_lst)\n",
    "                        # Write the rest of the data\n",
    "                        writer.writerows(data)\n",
    "\n",
    "                    print(f'Data has been scraped and saved to {csv_name}')\n",
    "\n",
    "                else:\n",
    "                    print(\"Table not found\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been scraped and saved to def_transfer_values_raw.csv\n",
      "Data has been scraped and saved to mid_transfer_values_raw.csv\n",
      "Data has been scraped and saved to atck_transfer_values_raw.csv\n",
      "Data has been scraped and saved to gkp_transfer_values_raw.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Web Scraping Player market value 2024\n",
    "mrkt_value_dict = {\n",
    "    \"https://www.transfermarkt.com/spieler-statistik/rekordmarktwerte/marktwertetop?position=Abwehr&land_id=0&plus=1\": \"def_transfer_values_raw.csv\",\n",
    "    \"https://www.transfermarkt.com/spieler-statistik/rekordmarktwerte/marktwertetop?position=Mittelfeld&land_id=0&plus=1\": \"mid_transfer_values_raw.csv\",\n",
    "    \"https://www.transfermarkt.com/spieler-statistik/rekordmarktwerte/marktwertetop?position=Sturm&land_id=0&plus=1\": \"atck_transfer_values_raw.csv\",\n",
    "    \"https://www.transfermarkt.com/spieler-statistik/rekordmarktwerte/marktwertetop?position=Torwart&land_id=0&plus=1\": \"gkp_transfer_values_raw.csv\"\n",
    "}\n",
    "\n",
    "for url, csv in mrkt_value_dict.items():\n",
    "    # Base URL of the Transfermarkt page\n",
    "    base_url = url\n",
    "    # Headers to mimic a browser visit\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    # Function to scrape a single page\n",
    "    def scrape_page(url):\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        table = soup.find(\"table\", {\"class\": \"items\"})\n",
    "        \n",
    "        players = []\n",
    "        ages = []\n",
    "        dates = []\n",
    "        record_values = []\n",
    "\n",
    "        if table:\n",
    "            for row in table.find_all(\"tr\")[1:]:  # Skip the header row\n",
    "                try:\n",
    "                    player = row.select_one('td:nth-of-type(2) .hauptlink')\n",
    "                    player_text = player.text.strip() if player else 'N/A'\n",
    "                    if player_text in players:\n",
    "                        continue  # Skip duplicates\n",
    "                    players.append(player_text)\n",
    "                except AttributeError:\n",
    "                    players.append(None)\n",
    "\n",
    "                try:\n",
    "                    age = row.select_one('td:nth-of-type(5)')\n",
    "                    age_text = age.text.strip() if age else 'N/A'\n",
    "                    ages.append(age_text)\n",
    "                except AttributeError:\n",
    "                    ages.append(None)\n",
    "\n",
    "                try:\n",
    "                    date = row.select_one('td:nth-of-type(6)')\n",
    "                    date_text = date.text.strip() if date else 'N/A'\n",
    "                    dates.append(date_text)\n",
    "                except AttributeError:\n",
    "                    dates.append(None)\n",
    "\n",
    "                try:\n",
    "                    record_mv = row.select_one('td:nth-of-type(7)')\n",
    "                    record_mv_text = record_mv.text.strip() if record_mv else 'N/A'\n",
    "                    record_values.append(record_mv_text)\n",
    "                except AttributeError:\n",
    "                    record_values.append(None)\n",
    "\n",
    "        return players, ages, dates, record_values\n",
    "\n",
    "    # Initialize lists to store all players and values\n",
    "    all_players = []\n",
    "    all_ages = []\n",
    "    all_dates = []\n",
    "    all_record_values = []\n",
    "\n",
    "    # Scrape the first page\n",
    "    players, ages, dates, record_values = scrape_page(base_url)\n",
    "    all_players.extend(players)\n",
    "    all_ages.extend(ages)\n",
    "    all_dates.extend(dates)\n",
    "    all_record_values.extend(record_values)\n",
    "\n",
    "    # Scrape up to 20 pages\n",
    "    for page in range(2, 21):\n",
    "        next_page_url = f\"{base_url}&page={page}\"\n",
    "        players, ages, dates, record_values = scrape_page(next_page_url)\n",
    "\n",
    "        if not players:  # If no players are found, break the loop\n",
    "            break\n",
    "\n",
    "        all_players.extend(players)\n",
    "        all_ages.extend(ages)\n",
    "        all_dates.extend(dates)\n",
    "        all_record_values.extend(record_values)\n",
    "\n",
    "    # Create a DataFrame to store the data\n",
    "    df = pd.DataFrame({\n",
    "        \"Player\": all_players,\n",
    "        \"Age\": all_ages,\n",
    "        \"Date\": all_dates,\n",
    "        \"Record MV\": all_record_values\n",
    "    })\n",
    "\n",
    "    # Save the data to a CSV file\n",
    "    df.to_csv(csv, index=False)\n",
    "    print(f'Data has been scraped and saved to {csv}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping Club Ranking Data\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://www.transfermarkt.co.uk/statistik/klubrangliste\"\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "}\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    table = soup.find(\"table\", {\"class\": \"items\"})\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    if table:\n",
    "        # Extract data\n",
    "        for row in table.find_all('tr', class_=['odd', 'even']):\n",
    "            club_name = row.find('a', title=True)['title']\n",
    "            country_name = row.find('img', class_='flaggenrahmen')['title']\n",
    "            values = [td.text.strip() for td in row.find_all('td', class_='rechts')]\n",
    "            overall = values.pop()\n",
    "            data.append([club_name, country_name] + values + [overall])\n",
    "    \n",
    "    return data\n",
    "\n",
    "all_teams = []\n",
    "\n",
    "# Scrape the first page\n",
    "data = scrape_page(base_url)\n",
    "all_teams.extend(data)\n",
    "\n",
    "# Scrape up to 20 pages\n",
    "for page in range(2, 23):\n",
    "    next_page_url = f\"{base_url}?page={page}\"\n",
    "    data = scrape_page(next_page_url)\n",
    "    all_teams.extend(data)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_teams, columns=['Club Name', 'Country', '20/21', '21/22', '22/23', '23/24', '24/25', 'Overall'])\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('club_rank_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping International Ranking Data\n",
    "\n",
    "# URLs\n",
    "url_2024 = 'https://www.transfermarkt.co.uk/statistik/weltrangliste/statistik/stat/plus/0/galerie/0?datum=2024-09-19'\n",
    "url_2023 = 'https://www.transfermarkt.co.uk/statistik/weltrangliste/statistik/stat/plus/0/galerie/0?datum=2023-09-21'\n",
    "url_2022 = 'https://www.transfermarkt.co.uk/statistik/weltrangliste/statistik/stat/plus/0/galerie/0?datum=2022-09-20'\n",
    "\n",
    "url_lst = [url_2022, url_2023, url_2024]\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "}\n",
    "\n",
    "for url in url_lst:\n",
    "\n",
    "    dt = url[-10:]\n",
    "    print(yr)\n",
    "    # Function to scrape a single page\n",
    "    def scrape_page(url):\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        table = soup.find(\"table\", {\"class\": \"items\"})\n",
    "        \n",
    "        data = []\n",
    "\n",
    "        if table:\n",
    "            # Extract data\n",
    "            for row in table.find_all('tr', class_=['odd', 'even']):\n",
    "    \n",
    "                rank = row.find('td', class_='zentriert cp').text.strip().split()[0]\n",
    "                previous_rank = row.find('span', class_='icons_sprite')['title'].split(': ')[1]\n",
    "                nation = row.find('a', title=True)['title']\n",
    "                confederation = row.find_all('td', class_='zentriert')[1].text.strip()\n",
    "                points = row.find('td', class_='zentriert hauptlink').text.strip()\n",
    "    \n",
    "                data.append([rank, previous_rank, nation, confederation, points])\n",
    "\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    all_teams = []\n",
    "\n",
    "    # Scrape the first page\n",
    "    data = scrape_page(url)\n",
    "    all_teams.extend(data)\n",
    "    print(data)\n",
    "\n",
    "   # Scrape up to 20 pages\n",
    "    for num in range(2, 10):\n",
    "        try:\n",
    "            next_page_url = f\"https://www.transfermarkt.co.uk/statistik/weltrangliste/statistik/stat/ajax/yw1/datum/{dt}/plus/0/galerie/0/page/{num}\"\n",
    "            data = scrape_page(next_page_url)\n",
    "            all_teams.extend(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: Page Not Found\")\n",
    "            break\n",
    "\n",
    "    print(all_teams)\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_teams, columns=['nation_rank','nation_prev_rank', 'nation', 'confederation', 'points'])\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(f'nation_rank_data_{dt}_raw.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping Competition Data\n",
    "\n",
    "dict_list = [\n",
    "    {\"url\": \"https://fbref.com/en/comps/season/2021-2022\", 'id': \"comps_intl_club_cup\"},\n",
    "    {\"url\": \"https://fbref.com/en/comps/season/2021-2022\", 'id': \"comps_fa_club_cup\"},\n",
    "    {\"url\": \"https://fbref.com/en/comps/season/2021-2022\", 'id': \"comps_1_fa_club_league_senior\"},\n",
    "    {\"url\": \"https://fbref.com/en/comps/season/2022-2023\", 'id': \"comps_intl_club_cup\"},\n",
    "    {\"url\": \"https://fbref.com/en/comps/season/2022-2023\", 'id': \"comps_fa_club_cup\"},\n",
    "    {\"url\": \"https://fbref.com/en/comps/season/2022-2023\", 'id': \"comps_1_fa_club_league_senior\"},\n",
    "    {\"url\": \"https://fbref.com/en/comps/season/2023-2024\", 'id': \"comps_intl_club_cup\"},\n",
    "    {\"url\": \"https://fbref.com/en/comps/season/2023-2024\", 'id': \"comps_fa_club_cup\"},\n",
    "    {\"url\": \"https://fbref.com/en/comps/season/2023-2024\", 'id': \"comps_1_fa_club_league_senior\"}\n",
    "]\n",
    "\n",
    "# Set up Selenium options\n",
    "options = Options()\n",
    "options.headless = True  # Run in headless mode (no GUI)\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Fetch the webpages\n",
    "for entry in dict_list:\n",
    "    url = entry[\"url\"]\n",
    "    url_str = str(url)\n",
    "    class_id = entry[\"id\"]\n",
    "    id = entry['id']\n",
    "    driver.get(url)\n",
    "\n",
    "    # Get the page source after JavaScript has rendered\n",
    "    html = driver.page_source\n",
    "\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find the table with the specified class\n",
    "    table = soup.find('table', {'id': class_id})\n",
    "\n",
    "    # Extract the data from the table\n",
    "    if table:\n",
    "\n",
    "        # Extract column headers with aria-label\n",
    "        headers = table.find_all('th')\n",
    "        header_labels = [header.get('aria-label') for header in headers if header.get('aria-label')]\n",
    "        # header_labels =  list(set(header_labels))\n",
    "\n",
    "        rows = table.find_all('tr')\n",
    "        data = []\n",
    "        \n",
    "        for row in rows:\n",
    "            # Extract the text from the <th> element\n",
    "            league_name = row.find('th').text.strip()\n",
    "            # Extract the text from the <td> elements\n",
    "            columns = row.find_all('td')\n",
    "            row_data = [league_name] + [column.text.strip() for column in columns]\n",
    "            \n",
    "            data.append(row_data)\n",
    "\n",
    "        # Creating output csv name\n",
    "        # Function to create the filename\n",
    "        def create_filename(url, id):\n",
    "            # Extract the season from the URL\n",
    "            match = re.search(r'/(\\d{4}-\\d{4})', url)\n",
    "            if match:\n",
    "                season = match.group(1).replace('-', '_')\n",
    "                # Format the filename\n",
    "                filename = f\"{id}_{season}_raw.csv\"\n",
    "                return filename\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "        csv_name = create_filename(url_str, id)\n",
    "        # Write data to CSV\n",
    "        with open(csv_name, 'w', newline='', encoding='utf-8') as file:\n",
    "        \n",
    "            writer = csv.writer(file)\n",
    "            # Write header labels first\n",
    "            writer.writerow(header_labels)\n",
    "            # Write the rest of the data\n",
    "            writer.writerows(data)\n",
    "\n",
    "        print(f'Data has been scraped and saved to {csv_name}')\n",
    "\n",
    "    else:\n",
    "        print(\"Table not found\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the Wikipedia page\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_FIFA_country_codes'\n",
    "\n",
    "# Send a request to fetch the HTML content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all tables on the page\n",
    "tables = soup.find_all('table', {'class': 'wikitable'})\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# Loop through all tables and concatenate each to the final DataFrame\n",
    "for table in tables:\n",
    "    df = pd.read_html(str(table))[0]\n",
    "    final_df = pd.concat([final_df, df], ignore_index=True)\n",
    "\n",
    "# Save the final concatenated DataFrame to a CSV file\n",
    "final_df.to_csv('raw_data/nation_raw_data/fifa_country_codes.csv', index=False)\n",
    "\n",
    "print(\"All tables have been scraped, concatenated, and saved as a single CSV file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for the raw data, the key is the wbesite containing the raw data and the key is the ID relating to the table required on the web page\n",
    "\n",
    "def league_table(seasons):\n",
    "\n",
    "    lst_of_dicts = []\n",
    "    dict = {}\n",
    "\n",
    "    for season in seasons:\n",
    "    \n",
    "        spa_urls = f'https://fbref.com/en/comps/12/{season}/{season}-La-Liga-Stats'\n",
    "        fra_urls = f'https://fbref.com/en/comps/13/{season}/{season}-Ligue-1-Stats'\n",
    "        eng_urls = f'https://fbref.com/en/comps/9/{season}/{season}-Premier-League-Stats'\n",
    "        ger_urls = f'https://fbref.com/en/comps/20/{season}/{season}-Bundesliga-Stats'\n",
    "        por_urls = f'https://fbref.com/en/comps/32/{season}/{season}-Primeira-Liga-Stats'\n",
    "        ita_urls = f'https://fbref.com/en/comps/11/{season}/{season}-Serie-A-Stats'\n",
    "        bul_urls = f'https://fbref.com/en/comps/67/{season}/{season}-Bulgarian-First-League-Stats'\n",
    "        aus_urls = f'https://fbref.com/en/comps/56/{season}/{season}-Austrian-Bundesliga-Stats'\n",
    "        dan_urls = f'https://fbref.com/en/comps/50/{season}/{season}-Danish-Superliga-Stats'\n",
    "        bel_urls = f'https://fbref.com/en/comps/37/{season}/{season}-Belgian-Pro-League-Stats'           \n",
    "        gre_urls = f'https://fbref.com/en/comps/27/{season}/{season}-Super-League-Greece-Stats'           \n",
    "        net_urls = f'https://fbref.com/en/comps/23/{season}/{season}-Eredivisie-Stats'\n",
    "        pol_urls = f'https://fbref.com/en/comps/36/{season}/{season}-Ekstraklasa-Stats'           \n",
    "        ser_urls = f'https://fbref.com/en/comps/54/{season}/{season}-Serbian-SuperLiga-Stats'\n",
    "        tur_urls = f'https://fbref.com/en/comps/26/{season}/{season}-Super-Lig-Stats'\n",
    "\n",
    "        url_lst = [ spa_urls, fra_urls, eng_urls, ger_urls, por_urls, ita_urls, bul_urls, aus_urls, dan_urls, bel_urls, gre_urls, net_urls, pol_urls, ser_urls, tur_urls ]\n",
    "\n",
    "        for url in url_lst:\n",
    "            match = re.search(r'comps/(\\d+)/', url)\n",
    "            match = match.group(1)\n",
    "\n",
    "            team_league_id = {'id': {f'results{season}{match}1_overall'}}\n",
    "\n",
    "            dict[url] = team_league_id\n",
    "                \n",
    "        lst_of_dicts.append(dict)\n",
    "                \n",
    "\n",
    "    return lst_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping player data\n",
    "\n",
    "# Set up Selenium options\n",
    "options = Options()\n",
    "options.headless = True  # Run in headless mode (no GUI)\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Function to create a unique filename for each CSV\n",
    "def create_filename(url):\n",
    "    \n",
    "    # Extract the relevant parts of the URL\n",
    "    name = url.split('/')[-1].replace('-', '_') + \"_table_placings.csv\"\n",
    "    return name\n",
    "\n",
    "# Fetch the webpages for the desired seasons\n",
    "for data_dict in league_table(['2021-2022', '2022-2023', '2023-2024']):\n",
    "    \n",
    "    for page, id in data_dict.items():\n",
    "\n",
    "        print(page, id)\n",
    "\n",
    "        csv_files = glob.glob(f'**.csv')\n",
    "\n",
    "        csv_name = create_filename(page)\n",
    "\n",
    "        if csv_name in csv_files:\n",
    "            print('Dataset already exists')\n",
    "\n",
    "        else:\n",
    "            driver.get(page)\n",
    "\n",
    "            # Get the page source after JavaScript has rendered\n",
    "            html = driver.page_source\n",
    "\n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            # Find the table with the specified class\n",
    "            table = soup.find('table', id)\n",
    "\n",
    "            # Extract the data from the table\n",
    "            if table:\n",
    "\n",
    "                # Find the thead element\n",
    "                thead = table.find('thead')\n",
    "                # Extract column headers with aria-label\n",
    "                headers = thead.find_all('th')\n",
    "                header_labels = [header.get('aria-label', header.text).strip() for header in headers if header.get('aria-label', header.text).strip()]\n",
    "                        \n",
    "                # Use a set to remove duplicates, required due to multiple headers in the table across the web page\n",
    "                unique_header_labels = list(dict.fromkeys(header_labels))\n",
    "\n",
    "                # Removing unwanted blank fields scraped incorrectly\n",
    "                if 'Rank' in unique_header_labels:\n",
    "                    filtered_lst = [header for header in header_labels if not (header.isdigit())][1:]\n",
    "                else:\n",
    "                    filtered_lst = [header for header in header_labels if not (header.isdigit())]\n",
    "\n",
    "                rows = table.find_all('tr')\n",
    "\n",
    "                data = []\n",
    "\n",
    "                for row in rows:\n",
    "\n",
    "                    columns = row.find_all('td')\n",
    "                    \n",
    "                    row_data = [column.text for column in columns]\n",
    "                    data.append(row_data)\n",
    "\n",
    "                # Write data to CSV\n",
    "                with open(csv_name, 'w', newline='', encoding='utf-8') as file:\n",
    "                    \n",
    "                    writer = csv.writer(file)\n",
    "                    # Write header labels first\n",
    "                    writer.writerow(filtered_lst)\n",
    "                    # Write the rest of the data\n",
    "                    writer.writerows(data)\n",
    "\n",
    "                print(f'Data has been scraped and saved to {csv_name}')\n",
    "\n",
    "            else:\n",
    "                print(\"Table not found\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
